{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setup\n",
    "\n",
    "To make sure things are working and `hepdata_lib` is available, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/06\n"
     ]
    }
   ],
   "source": [
    "import hepdata_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your HEPData submission\n",
    "\n",
    "The `Submission` object represents the whole HEPData entry and thus carries the top-level meta data that is equally valid for all the tables and variables you may want to enter. The object is also used to create the physical submission files you will upload to the HEPData web interface.\n",
    "\n",
    "When using `hepdata_lib` to make an entry, you always need to create a `Submission` object. Let's do that now, and then add data to it step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Submission\n",
    "submission = Submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a `Submission` should contain details on the actual analysis such as it's abstract as well as links to the actual publication. The abstract should be in a plain text file. For `inspire` there's a special `record_id`, while for links to `arXiv` etc. one should use plain hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.read_abstract(\"abstract.txt\")\n",
    "#submission.add_link(\"Webpage with all figures and tables\", \"https://cms-results.web.cern.ch/cms-results/public-results/publications/B2G-16-029/\")\n",
    "#submission.add_link(\"arXiv\", \"http://arxiv.org/abs/arXiv:1802.09407\")\n",
    "#submission.add_record_id(1657397, \"inspire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding CalcHEP model and LHE headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.add_additional_resource(\"CalcHEP model.\",\"HNmodel.tar\", copy_file=True)\n",
    "submission.add_additional_resource(\"CalcHEP LHE headers.\",\"LHEheaders.tar\", copy_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a table/figure\n",
    "\n",
    "In HEPData, figures and table will both be `Table` objects. The example here shows reading a plain text file containing the signal effiency times acceptance as a function of resonance mass for different signal models. The file has been uploaded to the `example_files` directory. For your submission, create a new directory, e.g. using the analysis identifier.\n",
    "\n",
    "Let's have a look at the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECTION           mumujj   eejj\r\n",
      "---------           ------  -----\r\n",
      "Trigger             0.692   0.515\r\n",
      "LeadingLepton     0.692   0.515\r\n",
      "SubleadingLepton     0.692   0.515\r\n",
      "FatJet               0.407   0.305\r\n",
      "m(ll)                0.379   0.305\r\n"
     ]
    }
   ],
   "source": [
    "!head cutflowM500.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the mass value, the other columns contain the efficiency times acceptance values.\n",
    "\n",
    "Let's create the table/figure. First, we need to give it a name, which is usually just the identifier in the paper, here \"Figure 1\". The table also needs a description, which is usually the caption. You also need to describe the location, i.e. where to find it in the publication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Table\n",
    "table = Table(\"Cut-flow table\")\n",
    "table.description = \"Cut-flow table mN=0.5TeV, electron, muon channel, 2016.\"\n",
    "table.location = \"Additional material\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to provide more information on what is actually shown, which is done via `keywords`. The ones that are available can be taken from the documentation:\n",
    "- [Observables](https://hepdata-submission.readthedocs.io/en/latest/keywords/observables.html)\n",
    "- [Phrases](https://hepdata-submission.readthedocs.io/en/latest/keywords/phrases.html)\n",
    "- [Particles](https://hepdata-submission.readthedocs.io/en/latest/keywords/partlist.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.keywords[\"observables\"] = [\"ACC\", \"EFF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the file. For this purpose, `numpy` is very handy. Since the first two rows are the header, we skip them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt(\"cutflowM500.txt\", skiprows=2, usecols=range(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTXT = np.loadtxt(\"cutflowM500.txt\", skiprows=2, usecols=range(0), dtype=\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` stores the content as arrays. You can actually see that the entry that was labelled as `NaN` is correctly read in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.692 0.515]\n",
      " [0.692 0.515]\n",
      " [0.692 0.515]\n",
      " [0.407 0.305]\n",
      " [0.379 0.305]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use this for our `Variable` definitions. The x-axis is usually the independent variable (`is_independent=True`), whereas the other ones are dependent (i.e. a function of the former). You also need to declare whether the variable is binned or not as well as the units. Similar as for the `keywords` used above, it is again important to provide additional information that can be found via the HEPData web interface using the observables and particles linked above. The values assigned are just slices of the `data` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Variable\n",
    "d = Variable(\"Selection\", is_independent=True, is_binned=False, units=\"\")\n",
    "d.values = dataTXT[:,0]\n",
    "\n",
    "Effmumujj = Variable(\"Efficiency mumujj\", is_independent=False, is_binned=False, units=\"\")\n",
    "Effmumujj.values = data[:,0]\n",
    "Effmumujj.add_qualifier(\"SQRT(S)\", 13, \"TeV\")\n",
    "\n",
    "Effeejj = Variable(\"Efficiency eejj\", is_independent=False, is_binned=False, units=\"\")\n",
    "Effeejj.values = data[:,1]\n",
    "Effeejj.add_qualifier(\"SQRT(S)\", 13, \"TeV\")\n",
    "\n",
    "table.add_variable(d)\n",
    "table.add_variable(Effmumujj)\n",
    "table.add_variable(Effeejj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all that's needed for the table/figure. We still need it to the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.add_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've added all tables/figures and the general submission details, you should add a few more keywords to all tables for better identification and searchability, e.g. the centre-of-mass energy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in submission.tables:\n",
    "    table.keywords[\"cmenergies\"] = [13000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading histograms for SR plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electron channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Table\n",
    "table = Table(\"Figure 4a\")\n",
    "table.description = \"Distributions of \\mllj for the data, and the pre-fit backgrounds (stacked histograms), in the SRs of the \\eeqq channel. The template for one signal hypothesis is shown overlaid as a yellow solid line. The overflow is included in the last bin. The middle panels show ratios of the data to the pre-fit background prediction and post-fit background yield as red open squares and blue points, respectively. The gray band in the middle panels indicates the systematic component of the post-fit uncertainty. The lower panels show the distributions of the pulls, defined in the text.\"\n",
    "table.location = \"Data from Figure 4 (upper left).\"\n",
    "table.keywords[\"observables\"] = [\"N\"]\n",
    "table.add_image(\"Figure_004-a.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import RootFileReader\n",
    "\n",
    "reader = RootFileReader(\"eejj_PostFit_histograms_L13_M05.root\")\n",
    "reader_data = RootFileReader(\"eejj_PostFit_histograms_L13_M05.root\")\n",
    "reader_signal = RootFileReader(\"eejj_PostFit_histograms_L13_M05.root\")\n",
    "\n",
    "TotalBackground = reader.read_hist_1d(\"prefit/TotalBkg\")\n",
    "#TT = reader.read_hist_1d(\"shapes_prefit/cat0_singleH/TT\")\n",
    "#QCD = reader.read_hist_1d(\"shapes_prefit/cat0_singleH/QCDTT\")\n",
    "#WJets = reader.read_hist_1d(\"shapes_prefit/cat0_singleH/WJets\")\n",
    "#ZJets = reader.read_hist_1d(\"shapes_prefit/cat0_singleH/ZJets\")\n",
    "\n",
    "Data = reader_data.read_hist_1d(\"prefit/data_obs\")\n",
    "\n",
    "signal = reader_signal.read_hist_1d(\"prefit/TotalSig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Variable, Uncertainty\n",
    "\n",
    "# x-axis: B quark mass\n",
    "mmed = Variable(\"$m(eeJ)$\", is_independent=True, is_binned=False, units=\"TeV\")\n",
    "mmed.values = signal[\"x\"]\n",
    "\n",
    "# y-axis: N events\n",
    "sig = Variable(\"Number of signal events\", is_independent=False, is_binned=False, units=\"\")\n",
    "sig.values = signal[\"y\"]\n",
    "\n",
    "totalbackground = Variable(\"Number of background events\", is_independent=False, is_binned=False, units=\"\")\n",
    "totalbackground.values = TotalBackground[\"y\"]\n",
    "\n",
    "#tt = Variable(\"Number of ttbar events\", is_independent=False, is_binned=False, units=\"\")\n",
    "#tt.values = TT[\"y\"]\n",
    "\n",
    "#qcd = Variable(\"Number of qcd events\", is_independent=False, is_binned=False, units=\"\")\n",
    "#qcd.values = QCD[\"y\"]\n",
    "\n",
    "#wjets = Variable(\"Number of wjets events\", is_independent=False, is_binned=False, units=\"\")\n",
    "#wjets.values = WJets[\"y\"]\n",
    "\n",
    "#zjets = Variable(\"Number of zjets events\", is_independent=False, is_binned=False, units=\"\")\n",
    "#zjets.values = ZJets[\"y\"]\n",
    "\n",
    "data = Variable(\"Number of data events\", is_independent=False, is_binned=False, units=\"\")\n",
    "data.values = Data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hepdata_lib import Uncertainty\n",
    "\n",
    "unc_totalbackground = Uncertainty(\"total uncertainty\", is_symmetric=True)\n",
    "unc_totalbackground.values = TotalBackground[\"y\"]\n",
    "\n",
    "unc_data = Uncertainty(\"Poisson errors\", is_symmetric=True)\n",
    "unc_data.values = Data[\"y\"]\n",
    "\n",
    "totalbackground.add_uncertainty(unc_totalbackground)\n",
    "data.add_uncertainty(unc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.add_variable(mmed)\n",
    "table.add_variable(sig)\n",
    "table.add_variable(totalbackground)\n",
    "#table.add_variable(tt)\n",
    "#table.add_variable(qcd)\n",
    "#table.add_variable(zjets)\n",
    "#table.add_variable(wjets)\n",
    "table.add_variable(data)\n",
    "\n",
    "submission.add_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muon channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Table\n",
    "table = Table(\"Figure 4a\")\n",
    "table.description = \"Distributions of \\mllj for the data, and the pre-fit backgrounds (stacked histograms), in the SRs of the \\mmqq channel. The template for one signal hypothesis is shown overlaid as a yellow solid line. The overflow is included in the last bin. The middle panels show ratios of the data to the pre-fit background prediction and post-fit background yield as red open squares and blue points, respectively. The gray band in the middle panels indicates the systematic component of the post-fit uncertainty. The lower panels show the distributions of the pulls, defined in the text.\"\n",
    "table.location = \"Data from Figure 4 (upper right).\"\n",
    "table.keywords[\"observables\"] = [\"N\"]\n",
    "table.add_image(\"Figure_004-b.pdf\")\n",
    "\n",
    "\n",
    "from hepdata_lib import RootFileReader\n",
    "reader = RootFileReader(\"mumujj_PostFit_histograms_L13_M05.root\")\n",
    "reader_data = RootFileReader(\"mumujj_PostFit_histograms_L13_M05.root\")\n",
    "reader_signal = RootFileReader(\"mumujj_PostFit_histograms_L13_M05.root\")\n",
    "TotalBackground = reader.read_hist_1d(\"prefit/TotalBkg\")\n",
    "Data = reader_data.read_hist_1d(\"prefit/data_obs\")\n",
    "signal = reader_signal.read_hist_1d(\"prefit/TotalSig\")\n",
    "\n",
    "\n",
    "from hepdata_lib import Uncertainty\n",
    "unc_totalbackground = Uncertainty(\"total uncertainty\", is_symmetric=True)\n",
    "unc_totalbackground.values = TotalBackground[\"y\"]\n",
    "unc_data = Uncertainty(\"Poisson errors\", is_symmetric=True)\n",
    "unc_data.values = Data[\"y\"]\n",
    "totalbackground.add_uncertainty(unc_totalbackground)\n",
    "data.add_uncertainty(unc_data)\n",
    "\n",
    "\n",
    "table.add_variable(mmed)\n",
    "table.add_variable(sig)\n",
    "table.add_variable(totalbackground)\n",
    "table.add_variable(data)\n",
    "submission.add_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create the submission for the upload. Here, we choose `example_output` as output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: omitting 'errors' since all uncertainties are zero for bin 1 of variable 'Number of data events'.\n",
      "Note that bins with zero content should preferably be omitted completely from the HEPData table.\n",
      "Warning: omitting 'errors' since all uncertainties are zero for bin 2 of variable 'Number of data events'.\n",
      "Note that bins with zero content should preferably be omitted completely from the HEPData table.\n",
      "Warning: omitting 'errors' since all uncertainties are zero for bin 1 of variable 'Number of data events'.\n",
      "Note that bins with zero content should preferably be omitted completely from the HEPData table.\n",
      "Warning: omitting 'errors' since all uncertainties are zero for bin 2 of variable 'Number of data events'.\n",
      "Note that bins with zero content should preferably be omitted completely from the HEPData table.\n",
      "\t error - submission.yaml is invalid HEPData YAML.\n",
      "\t error - Duplicate table name: Figure 4a\n",
      "\t error - Duplicate table data_file: figure_4a.yaml\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The tar ball is not valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8a8506eb775d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"HN_output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mremove_old\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/hepdata_lib/__init__.py\u001b[0m in \u001b[0;36mcreate_files\u001b[0;34m(self, outdir, validate, remove_old)\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_submission_validator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0mfull_submission_validator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mis_archive_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The tar ball is not valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mUncertainty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The tar ball is not valid"
     ]
    }
   ],
   "source": [
    "outdir = \"HN_output\"\n",
    "submission.create_files(outdir,remove_old=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the working directory, you will now find a `submission.tar.gz` file, which you can use for uploading to your HEPData sandbox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls submission.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `example_output` directory will contain the generated `yaml` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls HN_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
